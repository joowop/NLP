{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1fb042e",
   "metadata": {},
   "source": [
    "# **Settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "692ca8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')                       # warning 출력 false\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6b4a68",
   "metadata": {},
   "source": [
    "# **3. Bert 모델 가져오기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ba0a95",
   "metadata": {},
   "source": [
    "## **1) Settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e52f1206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from ratsnlp.nlpbook.classification import ClassificationExample\n",
    "\n",
    "# data setting\n",
    "class CustomCorpus:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "                \n",
    "    def get_examples(self,data_root_path,mode):\n",
    "        self.data = pd.read_csv(f'./data2/{mode}.csv')\n",
    "\n",
    "        examples = []\n",
    "        for temp in range(self.data.shape[0]):\n",
    "            text_a = self.data.iloc[temp]['text']\n",
    "            text_b = None\n",
    "            label = self.data.iloc[temp]['label']\n",
    "            examples.append(ClassificationExample(text_a=text_a,text_b=text_b,label=label))\n",
    "\n",
    "        return examples\n",
    "\n",
    "    def get_labels(self):\n",
    "        return [0, 1, 2, 3, 4, 5, 6]\n",
    "    \n",
    "    def labels_info(self):\n",
    "        return self.encoder.classes_\n",
    "\n",
    "    @property\n",
    "    def num_labels(self):\n",
    "        return len(self.get_labels())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e993870d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set seed: 100\n"
     ]
    }
   ],
   "source": [
    "from ratsnlp.nlpbook.classification import ClassificationTrainArguments\n",
    "\n",
    "# model setting\n",
    "model_path = './kcbert2'\n",
    "if not os.path.isdir(model_path):\n",
    "    os.mkdir(model_path)\n",
    "\n",
    "# arguments\n",
    "args = ClassificationTrainArguments(\n",
    "    pretrained_model_name = 'beomi/kcbert-base',\n",
    "    downstream_corpus_name = 'data2',\n",
    "    downstream_corpus_root_dir = './',\n",
    "    downstream_model_dir = model_path,\n",
    "    batch_size=32 if torch.cuda.is_available() else 4,\n",
    "    learning_rate = 5e-5,\n",
    "    max_seq_length = 128,\n",
    "    seed = 100,\n",
    "    epochs = 5\n",
    ")\n",
    "\n",
    "from ratsnlp import nlpbook\n",
    "nlpbook.set_seed(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0a4f33",
   "metadata": {},
   "source": [
    "## **2) 토크나이저 불러오기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d246fa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    do_lower_case = False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dc1eaf",
   "metadata": {},
   "source": [
    "## **3) 데이터셋 만들기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0b0e453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터셋 불러오기\n",
    "from ratsnlp.nlpbook.classification import NsmcCorpus, ClassificationDataset\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "corpus = CustomCorpus()\n",
    "train_dataset = ClassificationDataset(\n",
    "    args = args,\n",
    "    corpus = corpus,\n",
    "    tokenizer = tokenizer,\n",
    "    mode = 'train'\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    sampler=RandomSampler(train_dataset,replacement=False),\n",
    "    collate_fn = nlpbook.data_collator,\n",
    "    drop_last = False\n",
    ")\n",
    "\n",
    "val_dataset = ClassificationDataset(\n",
    "    args = args,\n",
    "    corpus = corpus,\n",
    "    tokenizer = tokenizer,\n",
    "    mode = 'val'\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=args.batch_size,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    collate_fn = nlpbook.data_collator,\n",
    "    drop_last = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54a1dbb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassificationFeatures(input_ids=[2, 25802, 9319, 8099, 347, 9472, 7997, 11247, 26349, 11888, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6edfea12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassificationFeatures(input_ids=[2, 25802, 9319, 8099, 347, 9472, 7997, 11247, 26349, 11888, 17, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ff6e76",
   "metadata": {},
   "source": [
    "# **4. 모델 학습**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f25c6e8",
   "metadata": {},
   "source": [
    "## **1) 모델 생성 및 초기화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b9302389",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/kcbert-base were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at beomi/kcbert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertForSequenceClassification\n",
    "\n",
    "pretrained_model_config = BertConfig.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    num_labels = corpus.num_labels\n",
    ")\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    args.pretrained_model_name,\n",
    "    config = pretrained_model_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f7a71a",
   "metadata": {},
   "source": [
    "## **2) TASK 설정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e232f57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ratsnlp.nlpbook.classification import ClassificationTask\n",
    "task = ClassificationTask(model, args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf42070",
   "metadata": {},
   "source": [
    "## **3) 학습**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8c615fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bed52bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboardX in c:\\users\\user\\anaconda3\\envs\\bert\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\envs\\bert\\lib\\site-packages (from tensorboardX) (1.25.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\envs\\bert\\lib\\site-packages (from tensorboardX) (23.1)\n",
      "Requirement already satisfied: protobuf>=4.22.3 in c:\\users\\user\\anaconda3\\envs\\bert\\lib\\site-packages (from tensorboardX) (4.23.4)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "! pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "72710470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인스턴스 생성\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28975cbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                          | Params\n",
      "--------------------------------------------------------\n",
      "0 | model | BertForSequenceClassification | 108 M \n",
      "--------------------------------------------------------\n",
      "108 M     Trainable params\n",
      "0         Non-trainable params\n",
      "108 M     Total params\n",
      "435.696   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09a6d666928f4fb1805254c7d0fa2857",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19c24da22435415d8ca25efa40968d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = nlpbook.get_trainer(args)\n",
    "trainer.fit(\n",
    "    task,\n",
    "    train_dataloaders = train_dataloader,\n",
    "    val_dataloaders = val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7917de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 쓰기 종료\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fb700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './model5.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82bf2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f4abdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir='C:/Users/user/metaverse/project7/runs' --host=0.0.0.0 --port 6010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "780f5f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start! > batch 100 > batch 200 > batch 300 > batch 400 > batch 500 > batch 600 > batch 700 > batch 800 > batch 900 > batch 1000 > batch 1100 > batch 1200 > batch 1300 > batch 1400 > batch 1500 > batch 1600 > batch 1700 >>> End!\n",
      "Accuracy = 0.9928708576304935\n"
     ]
    }
   ],
   "source": [
    "val_data = pd.read_csv('./data2/val_data.csv')\n",
    "\n",
    "with torch.no_grad():\n",
    "    total_corr = 0\n",
    "    cnt = 0\n",
    "    print('Start!', end=' ')\n",
    "    for inputs in val_dataloader:\n",
    "        labels = inputs.pop('labels',None)\n",
    "        \n",
    "        outputs = model(**{k: torch.tensor(v) for k,v in inputs.items()})\n",
    "        prob = outputs.logits.softmax(dim=1)\n",
    "        _, preds = torch.max(prob, 1)\n",
    "        \n",
    "        corr = preds.eq(labels).sum().item()\n",
    "        total_corr += corr\n",
    "        \n",
    "        cnt += 1\n",
    "        if cnt % 100 == 0:\n",
    "            print(f'> batch {cnt}', end=' ')\n",
    "    print('>>> End!')\n",
    "\n",
    "print(f'Accuracy = {total_corr / val_data.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3f4cc3b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.9934753235670601\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy = {total_corr / val_data.shape[0]}') # good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c813c360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "58aea142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b32b964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86198fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d9fb70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d6d280",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb56ee55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed85c2d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
